{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BooksDistilled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRnHewVoYGuF",
        "colab_type": "text"
      },
      "source": [
        "# BooksDistilled\n",
        "\n",
        "Set up BooksDistilled from https://github.com/ruthschulz/Book_Summarizer and run on the provided example of Alice's Adventures in Wonderland, downloaded from Project Gutenberg and stored as 11.txt in data/raw_books."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQHCi1MeeCw",
        "colab_type": "text"
      },
      "source": [
        "## Setup BooksDistilled\n",
        "\n",
        "Clone from github, install requirements, additional nltk and spacy requirements, set up the abstractive summarizer (LeafNATS).\n",
        "\n",
        "Note that you need to upload the model and vocab for LeafNATS separately, links are provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLYu1L1hUOBa",
        "colab_type": "code",
        "outputId": "a528ae1e-4c41-414d-a252-b730ccc59c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!git clone https://github.com/ruthschulz/Book_Summarizer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Book_Summarizer'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Total 333 (delta 0), reused 0 (delta 0), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (333/333), 19.19 MiB | 13.94 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBT63m_sJjDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('Book_Summarizer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUpxSbFoUk60",
        "colab_type": "code",
        "outputId": "41142fb0-eaaa-44ed-9c3e-a83edc182f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget (from -r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting sumy (from -r requirements.txt (line 5))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.5MB/s \n",
            "\u001b[?25hCollecting fuzzywuzzy (from -r requirements.txt (line 6))\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.24.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.1.8)\n",
            "Collecting pyrouge (from -r requirements.txt (line 9))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 22.3MB/s \n",
            "\u001b[?25hCollecting python-Levenshtein (from -r requirements.txt (line 10))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (1.2.0)\n",
            "Collecting regex (from -r requirements.txt (line 12))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 44.1MB/s \n",
            "\u001b[?25hCollecting pycountry>=18.2.23 (from sumy->-r requirements.txt (line 5))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/b6/154fe93072051d8ce7bf197690957b6d0ac9a21d51c9a1d05bd7c6fdb16f/pycountry-19.8.18.tar.gz (10.0MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy->-r requirements.txt (line 5)) (0.6.2)\n",
            "Collecting breadability>=0.1.20 (from sumy->-r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy->-r requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy->-r requirements.txt (line 5)) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 7)) (1.16.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 7)) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (2.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (0.2.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (0.1.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 8)) (0.9.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein->-r requirements.txt (line 10)) (41.2.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy->-r requirements.txt (line 5)) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy->-r requirements.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 5)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 5)) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy->-r requirements.txt (line 8)) (4.28.1)\n",
            "Building wheels for collected packages: wget, pyrouge, python-Levenshtein, regex, pycountry, breadability\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=9c2eb7eef6f99da11364338d5998d9d6b8a500ffa5b63b5e29ad4138c9c2a64f\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=905a7b0908204ad71b4fc9074b024d5a7059cd13339448678c1195ae84f77788\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144664 sha256=5ac40d6667fd038c5dcc4349e0175a5501f4458d03ac9e3e25cee6b0fd34dc89\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609239 sha256=c220aa0ebadbe9c1fd52984cd5a23fe4dcd63959bfb45a20c6183ae4c154847a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-19.8.18-py2.py3-none-any.whl size=10627360 sha256=2d3e07ceb18188c0208814dcd1a809a157edf4845f17ef2fceba6ad0dbac515f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/98/bf/f0fa1c6bf8cf2cbdb750d583f84be51c2cd8272460b8b36bd3\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21682 sha256=e78a0b2e93feb0d828291246c794b401e9c0555ce103cc13aa2b839b413c3d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built wget pyrouge python-Levenshtein regex pycountry breadability\n",
            "Installing collected packages: wget, pycountry, breadability, sumy, fuzzywuzzy, pyrouge, python-Levenshtein, regex\n",
            "Successfully installed breadability-0.1.20 fuzzywuzzy-0.17.0 pycountry-19.8.18 pyrouge-0.1.3 python-Levenshtein-0.12.0 regex-2019.8.19 sumy-0.8.1 wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVQU06rZUZmV",
        "colab_type": "code",
        "outputId": "515c1396-8a56-457f-cb04-02aef8dd9276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import nltk; nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_7KZcbSUjIQ",
        "colab_type": "code",
        "outputId": "8507bfb2-4ce1-4dca-b1b2-8b8c4067f02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 53.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=675d9bd0d4146bd14f02653ce46ff1660f090a2c74a5459bae9bc7c4af14ae53\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ge3r33y9/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uqru5hjVyZu",
        "colab_type": "code",
        "outputId": "96605bdc-1c0d-4978-c44a-bf71495c2e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/ruthschulz/LeafNATS"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LeafNATS'...\n",
            "remote: Enumerating objects: 221, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 1482 (delta 79), reused 140 (delta 27), pack-reused 1261\u001b[K\n",
            "Receiving objects: 100% (1482/1482), 4.40 MiB | 6.47 MiB/s, done.\n",
            "Resolving deltas: 100% (734/734), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfxiRSuJWBT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv LeafNATS/* Book_Summarizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRLSOUqKRheF",
        "colab_type": "text"
      },
      "source": [
        "Upload leafnats-pointer-generator-model.zip and vocab from https://drive.google.com/file/d/1EuLYK3k-U65xMtazqYskt6A97ZLe2a-n/view?usp=sharing and https://drive.google.com/file/d/1Kn14TMg0-ZLpnAUyJVhcuLXVWzZCD0Yg/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX1jSYQzWnsC",
        "colab_type": "code",
        "outputId": "25224959-ed01-4734-98d1-47a80d91307a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!unzip ../leafnats-pointer-generator-model.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../leafnats-pointer-generator-model.zip\n",
            "  inflating: pgdecoder_0_0.model     \n",
            "  inflating: encoder2decoder_0_0.model  \n",
            "  inflating: encoder_0_0.model       \n",
            "  inflating: embedding_0_0.model     \n",
            "  inflating: decoder2proj_0_0.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0_UJzadW1WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv *.model nats_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w-LjPpHKUsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir sum_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RlFCs-0YUSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../vocab sum_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z2nImHjfcjI",
        "colab_type": "text"
      },
      "source": [
        "## Run BooksDistilled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdhtv0b5R9AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('Book_Summarizer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ADTRsJa5_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "40e2f15b-c35e-4014-8207-95927807f686"
      },
      "source": [
        "!python book_summarizer.py -h"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: book_summarizer.py [-h] [-b B] [-en] [-ex [EX]]\n",
            "                          [-exTechnique [EXTECHNIQUE]] [-ae] [-aa [AA]] [-fl]\n",
            "                          [-analysis] [-w]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -b B                  create a summary of a given book text file, indicated\n",
            "                        by number, if not included, all books in\n",
            "                        data/raw_books will be summarized\n",
            "  -en                   include an entity summary of each chapter\n",
            "  -ex [EX]              include an extractive summary of each chapter,\n",
            "                        optionally choose how many sentences up to 9 (default\n",
            "                        is 1)\n",
            "  -exTechnique [EXTECHNIQUE]\n",
            "                        choose the technique for extractive summarization by\n",
            "                        name, options: luhn, lsa, lexrank, textrank, sumbasic,\n",
            "                        kl, reduction, random\n",
            "  -ae                   include an abstractive summary from an extractive\n",
            "                        summary of each chapter\n",
            "  -aa [AA]              include an abstractive summary from an abstractive\n",
            "                        summary of each chapter, optionally choose short (s)\n",
            "                        or long (l) summary (default is short)\n",
            "  -fl                   include the first lines of each chapter\n",
            "  -analysis             analyze the summary\n",
            "  -w                    write over the existing summary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzcx8h5jaHab",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the first lines of each chapter, writing over the summary if it already exists. The summary file will be 11-fl.txt in the results/summaries directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Dn-34WZJZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python book_summarizer.py -fl -w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IniqPYSJaZNW",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the extracted entities for each chapter, writing over the summary if it already exists. The summary files will be 11-en.txt and 11.csv in the results/summaries directory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qetsRdBfX6JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python book_summarizer.py -en -w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnpSEkYxakSw",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the extractive summary for each chapter, writing over the summary if it already exists. The Luhn summarizer will be used, with 2 sentences output per chapter. The summary file will be 11-ex.txt in the results/summaries directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fsN3_igZMx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python book_summarizer.py -ex 2 -exTechnique luhn -w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juMhlFfqbBkF",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the abstractive summary from an extractive summary for each chapter, writing over the summary if it already exists. The LSA summarizer will be used. The summary file will be 11-ae.txt in the results/summaries directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbUAlQAoZPk8",
        "colab_type": "code",
        "outputId": "5451f773-ce05-4a6c-9085-37f391129c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python book_summarizer.py -ae -exTechnique lsa -w"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000470h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000421h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000381h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000381h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000417h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000386h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000382h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000386h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000414h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000384h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000387h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000390h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000419h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000383h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000381h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 1\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[                         ]0%0.000412h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1rsZ3gcJlb",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the abstractive summary from an abstractive summary for each chapter, writing over the summary if it already exists. A long summary (5-20 sentences) will be written per chapter. (The short default, s, will write 1-4 sentences per chapter.) The summary file will be 11-aa.txt in the results/summaries directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy4PDOy5ZSuq",
        "colab_type": "code",
        "outputId": "9727c494-ce28-4d89-b86c-c3948821cee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python book_summarizer.py -aa l -w"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.004999h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.005055h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 8\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>   ]88%0.003096h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 4\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>      ]75%0.001582h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005787h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 5\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>     ]80%0.001945h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 3\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>        ]67%0.001157h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 6\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>    ]83%0.002309h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 16\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>> ]94%0.006210h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 16\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>> ]94%0.006194h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005822h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005783h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 14\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005452h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 12\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.004641h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.005027h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbSp4a0cyq7",
        "colab_type": "text"
      },
      "source": [
        "Run the book summarizer including the first lines, entities, extractive summary using Luhn, and abstractive summary from an abstractive summary for each chapter. The summary file will be 11-fl-en-ex-aa.txt in the results/summaries directory. The analysis of the summary, comparing to the ground truth summary in data/summaries, will be 11.csv in the results/analysis directory. If the summary already exists it will not be re-created, but the analysis will be run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9R3_novcLO7",
        "colab_type": "code",
        "outputId": "ebebaac6-78c6-45ec-e27f-1ec9f90f2afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python book_summarizer.py -fl -en -ex 1 -exTechnique luhn -aa -analysis"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.004997h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.005017h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 8\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>   ]88%0.003108h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 4\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>      ]75%0.001571h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005770h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 5\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>     ]80%0.001931h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 3\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>        ]67%0.001153h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 6\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>    ]83%0.002325h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 16\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>> ]94%0.006198h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 16\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>> ]94%0.006172h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005800h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 15\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005928h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 14\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]93%0.005466h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 12\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.004656h\n",
            "The vocabulary size: 50000\n",
            "{}\n",
            "{'decoder2proj': Linear(in_features=256, out_features=128, bias=False),\n",
            " 'embedding': natsEmbedding(\n",
            "  (embedding): Embedding(50000, 128)\n",
            "  (proj2vocab): Linear(in_features=128, out_features=50000, bias=True)\n",
            "),\n",
            " 'encoder': natsEncoder(\n",
            "  (encoder): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
            "),\n",
            " 'encoder2decoder': natsEncoder2Decoder(\n",
            "  (encoder2decoder): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder2decoder_c): Linear(in_features=512, out_features=256, bias=True)\n",
            "),\n",
            " 'pgdecoder': PointerGeneratorDecoder(\n",
            "  (rnn_): LSTMCell(384, 256)\n",
            "  (encoder_attn_layer): AttentionEncoder(\n",
            "    (attn_en_in): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_cv_in): Linear(in_features=1, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (decoder_attn_layer): AttentionDecoder(\n",
            "    (attn_en_in): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (attn_de_in): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (attn_warp_in): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (attn_out): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (pt_out): Linear(in_features=1152, out_features=1, bias=True)\n",
            ")}\n",
            "The number of batches (test): 13\n",
            "You choose to use *_0_0.model for decoding.\n",
            "[>>>>>>>>>>>>>>>>>>>>>>>  ]92%0.005206h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6es1AkIYgq41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7259ec2-dee0-41cd-f0b9-a3084ff57c53"
      },
      "source": [
        "!cat ../results/summaries/11-fl-en-ex-aa.txt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters: Alice, Hatter, Gryphon, Dormouse, Mouse, the March Hare, Queen, Dinah, Bill, the White Rabbit.\n",
            "Key terms: the Mock Turtle, Duchess, Caterpillar, Pigeon, Footman, English, CHAPTER II, French, earth, Eaglet.\n",
            "\n",
            "Chapter 0\n",
            "Starting:\n",
            "ALICE'S ADVENTURES IN WONDERLAND Lewis Carroll ...\n",
            "Characters: Alice, Dinah, the White Rabbit, Down, Latitude.\n",
            "Key terms: Longitude, 'Call the first witness,', TOOK A WATCH, earth, New Zealand.\n",
            "Quote: \"(when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\"\n",
            "The rabbit went straight on like a tunnel for some way. 'i shall think nothing of tumbling down stairs! (alice had no idea what latitude was, or longitude either. (miss me very much to - night, i should think! there was not a moment to be lost, and the white rabbit was still in sight. alice opened the door and found that it led into a small passage. alice,), round the neck of the bottle was a paper label, with the words' drink me 'beautifully printed on it in large letters. alice ventured to taste it, and finding it very nice, (it had taught them: such as, that a red - hot poker will burn you if you hold it too long. she was only ten inches high when she got to the door when she got to the door. 'i advise you to leave off this minute!''i'll eat it,' said alice, 'a very small cake'  'i'll eat it,' said alice, 'a very small cake'  she set to work, and very soon finished off the cake.\n",
            "Chapter 1\n",
            "Starting:\n",
            "CHAPTER II. The Pool of Tears 'Curiouser and curiouser!' cried Alice (she was so much surprised, that ...\n",
            "Characters: Alice, Mouse, Mabel, Curiouser, ALICE.\n",
            "Key terms: 'Call the first witness,', English, Paris, Rome, French.\n",
            "Quote: \"Just then she heard something splashing about in the pool a little way off, and she swam nearer to make out what it was: at first she thought it must be a walrus or hippopotamus, but then she remembered how small she was now, and she soon made out that it was only a mouse that had slipped in like herself.\"\n",
            "The pool of tears' curiouser and curiouser! alice's right foot, esq, near the fender, (with alice's love). alice felt so desperate that she was ready to ask help of any one. 'i almost think i can remember feeling a little different,' she said. london is the capital of paris, and paris is the capital of rome, and rome -- no, that's all wrong, i'm certain! poor alice and her eyes filled with tears again as she went on. alice, a good deal frightened at the sudden change, said alice, a good deal frightened at the sudden change. (alice had been to the seaside once in her life. (alice thought this must be the right way of speaking to a mouse. 'i think you'd take a fancy to cats if you could only see her. alice, in a great hurry to change the subject of conversation. the mouse was swimming away from her as hard as it could go. alice led the way, and the whole party swam to the shore.\n",
            "Chapter 2\n",
            "Starting:\n",
            "CHAPTER III. A Caucus-Race and a Long Tale They were indeed a queer-looking party that assembled on the bank--the ...\n",
            "Characters: Alice, Dodo, Mouse, Lory, William.\n",
            "Key terms: Caucus, 'Call the first witness,', English, Northumbria--, Eaglet.\n",
            "Quote: \"Indeed, she had quite a long argument with the Lory, who at last turned sulky, and would only say, 'I am older than you, and must know better'; and this Alice would not allow without knowing how old it was, and, as the Lory positively refused to tell its age, there was no more to be said.\"\n",
            "The first question of course was, how to get dry again. alice kept her eyes anxiously fixed on it, for she felt sure she would catch a bad cold if she did not get dry very soon. william's conduct at first was moderate. alice in a melancholy tone: 'i don't know the meaning of half those long words, and, what's more, i don't believe you do either!'there was no 'one, two, three, and away, and away,' but they began running. alice had no idea what to do, and in despair she put her hand in her pocket. the large birds complained that they could not taste theirs. the mouse to the cur, \"such a trial, dear sir, with no jury or judge, would be wasting our breath.\n",
            "Chapter 3\n",
            "Starting:\n",
            "'You are not attending!' said the Mouse to Alice severely. 'What are you thinking of?' ...\n",
            "Characters: Alice, Mouse, Dinah, Lory, Crab.\n",
            "Key terms: Magpie.\n",
            "Quote: \"In a little while, however, she again heard a little pattering of footsteps in the distance, and she looked up eagerly, half hoping that the Mouse had changed his mind, and was coming back to finish his story.\"\n",
            "Alice very humbly: 'I beg your pardon'  'I shall do nothing of the sort,' said alice, always ready to make herself useful. 'I wish I had our dinah here, I know I do!'This speech caused a remarkable sensation among the party. The mouse had changed his mind, and was coming back to finish his story. \n",
            "Chapter 4\n",
            "Starting:\n",
            "CHAPTER IV. The Rabbit Sends in a Little Bill It was the White Rabbit, trotting slowly back again, and looking ...\n",
            "Characters: Alice, Bill, Mary Ann, Dinah, the White Rabbit.\n",
            "Key terms: 'Call the first witness,', Duchess, Panther.\n",
            "Quote: \"Hardly knowing what she did, she picked up a little bit of stick, and held it out to the puppy; whereupon the puppy jumped into the air off all its feet at once, with a yelp of delight, and rushed at the stick, and made believe to worry it; then Alice dodged behind a great thistle, to keep herself from being run over; and the moment she appeared on the other side, the puppy made another rush at the stick, and tumbled head over heels in its hurry to get hold of it; then Alice, thinking it was very like having a game of play with a cart-horse, and expecting every moment to be trampled under its feet, ran round the thistle again; then the puppy began a series of short charges at the stick, running a very little way forwards each time and a long way back, and barking hoarsely all the while, till at last it sat down a good way off, panting, with its tongue hanging out of its mouth, and its great eyes half shut.\"\n",
            "The rabbit sends in a little bill it was looking for the fan and the pair of white kid gloves. 'i'd better take him his fan and gloves -- that is, if i can find them'  'i'd better take him his fan and gloves -- that is, if i can find them'  'i'd better take him his fan and gloves -- that is, if i can find them,' she said. \"\" had a fan and two or three pairs of tiny white kid gloves. 'i know something interesting is sure to happen,' she said to herself. alice, the little magic bottle had its full effect. alice knew it was the rabbit coming to look for her. alice heard it say to itself 'then i'll go round and get in at the window'  'i'll go round and get in at the window,' said the rabbit angrily. 'i don't like it, yer honour:' i don't like it, yer honour! the other ladder?--why will the roof bear?--mind that loose slate -- oh, it's coming down! so bill's got to come down the chimney, has he? 'i'm better now -- but i'm a deal too flustered to tell you,' said the rabbit's voice. the poor little lizard, bill, was in the middle, being held up by two guinea - pigs, who were giving it something out of a bottle. an enormous puppy was looking down at her with large round eyes. whereupon the puppy jumped into the air off all its feet at once, with a yelp of delight, and rushed at the stick, and tumbled head over heels in its hurry to get hold of it. there was a large mushroom growing near her, about the same height as herself.\n",
            "Chapter 5\n",
            "Starting:\n",
            "CHAPTER V. Advice from a Caterpillar The Caterpillar and Alice looked at each other for some time in silence: ...\n",
            "Characters: Alice, William, WILLIAM.\n",
            "Key terms: Caterpillar, 'Call the first witness,'.\n",
            "Quote: \"'Well, perhaps you haven't found it so yet,' said Alice; 'but when you have to turn into a chrysalis--you will some day, you know--and then after that into a butterfly, I should think you'll feel it a little queer, won't you?'\"\n",
            "Alice replied, rather shyly, 'I can't explain myself, I'm afraid, sir'  'I can't explain myself, I'm afraid, sir' said the caterpillar sternly. Alice replied very politely, 'for I can't understand it myself to begin with; and being so many different sizes in a day is very confusing'  'I should think you'll feel it a little queer, won't you?'Alice thought she might as well wait, as she had nothing else to do, and perhaps after all it might tell her something worth hearing. Alice folded her hands, and began:-- 'you are old, father william, \"' said the caterpillar. 'I kept all my limbs very supple by the use of this ointment,' said his father, 'I took to the law, and argued each case with my wife; and the muscular strength, which it gave to my jaw, has lasted the rest of my life'  'I kept all my limbs very supple by the use of this ointment -- one shilling the box-- allow me to sell you a couple?'\n",
            "Chapter 6\n",
            "Starting:\n",
            "'That is not said right,' said the Caterpillar. 'Not QUITE right, I'm afraid,' said Alice, timidly; 'some of the words ...\n",
            "Characters: Alice.\n",
            "Key terms: Caterpillar.\n",
            "Quote: \"She was a good deal frightened by this very sudden change, but she felt that there was no time to be lost, as she was shrinking rapidly; so she set to work at once to eat some of the other bit.\"\n",
            "'I should like to be a little larger, sir, if you wouldn't mind,' said the caterpillar. 'I wish the creatures wouldn't be so easily offended!'Alice remained looking thoughtfully at the mushroom for a minute. \n",
            "Chapter 7\n",
            "Starting:\n",
            "  *    *    *    *    *    *    *     *    *    *    *    *    * ...\n",
            "Characters: Alice.\n",
            "Key terms: Pigeon.\n",
            "Quote: \"She had just succeeded in curving it down into a graceful zigzag, and was going to dive in among the leaves, which she found to be nothing but the tops of the trees under which she had been wandering, when a sharp hiss made her draw back in a hurry: a large pigeon had flown into her face, and was beating her violently with its wings.\"\n",
            "'I can't see you?'A sharp hiss made her draw back in a hurry in a graceful zigzag. Alice was more and more puzzled, but she thought there was no use in saying anything more till the pigeon had finished. Alice, who was a very truthful child, said the pigeon in a tone of the deepest contempt. 'I shouldn't want yours: I shouldn't want yours: I don't like them raw'  after a while she remembered that she still held the pieces of mushroom in her hands. 'I've got back to my right size: the next thing is, to get into that beautiful garden,' she said. \n",
            "Chapter 8\n",
            "Starting:\n",
            "CHAPTER VI. Pig and Pepper For a minute or two she stood looking at the house, and wondering what ...\n",
            "Characters: Alice, the March Hare, Cheshire.\n",
            "Key terms: Duchess, Footman, 'Call the first witness,'.\n",
            "Quote: \"For a minute or two she stood looking at the house, and wondering what to do next, when suddenly a footman in livery came running out of the wood--(she considered him to be a footman because he was in livery: otherwise, judging by his face only, she would have called him a fish)--and rapped loudly at the door with his knuckles.\"\n",
            "Pig and pepper for a minute or two she stood looking at the house. alice laughed so much at this, that she had to run back into the wood for fear of their hearing her. 'i shall sit here,' said alice, 'how am i to get in?''i shall sit here,' said the footman, and began whistling. the duchess was sitting on a three - legged stool in the middle, nursing a baby; the cook was leaning over the fire, stirring a large cauldron which seemed to be full of soup. alice said very politely, feeling quite pleased to have got into a conversation. the duchess said in a hoarse growl, 'the world would go round a deal faster than it does'  the duchess said in a hoarse growl,' the world would go round a deal faster than it does. the duchess said to alice, flinging the baby at her as she spoke. the cook threw a frying - pan after her as she went out. alice, alice, said the last words out loud, and the little thing grunted in reply (it had left off sneezing by this time). alice was a little startled by seeing the cheshire cat sitting on a bough of a tree a few yards off. 'i don't much care where--' said the cat. alice felt that this could not be denied, so she tried another question. 'i call it purring, not growling,' said the cat. alice waited a little, half expecting to see it again, but it did not appear, and after a minute or two she walked on in the direction in which the march hare was said to live. the chimneys were shaped like ears and the roof was thatched with fur.\n",
            "Chapter 9\n",
            "Starting:\n",
            "CHAPTER VII. A Mad Tea-Party There was a table set out under a tree in front of the house, and the ...\n",
            "Characters: Alice, Hatter, Dormouse, the March Hare, Time.\n",
            "Key terms: 'Call the first witness,', MINE, English.\n",
            "Quote: \"The Dormouse had closed its eyes by this time, and was going off into a doze; but, on being pinched by the Hatter, it woke up again with a little shriek, and went on: '--that begins with an M, such as mouse-traps, and the moon, and memory, and muchness--you know you say things are \"much of a muchness\"--did you ever see such a thing as a drawing of a muchness?'\"\n",
            "A dormouse was sitting between them, fast asleep, and the other two were using it as a cushion, resting their elbows on it, and talking over its head. the march hare said the march hare wasn't very civil of you to offer it. the march hare went on. alice thought over all she could remember about ravens and writing. alice had been looking over his shoulder with some curiosity. 'i think you might do something better with the time,' said the march hare. the march hare said to itself in a whisper. 'i'd hardly finished the first verse,' when the queen jumped up and bawled out, \"he's murdering the time! the hatter went on in a mournful tone. 'i wasn't asleep,' he said in a hoarse, feeble voice: 'i heard every word you fellows were saying'  'i wasn't asleep,' he said in a hoarse, feeble voice: 'i heard every word you fellows were saying'  'i heard every word you fellows were saying,' he said in a hoarse, feeble voice: 'i heard every word you fellows were saying.'alice tried to fancy to herself what such an extraordinary ways of living would be like, but why did they live at the bottom of a well? 'i won't interrupt again. the march hare moved into the dormouse's place, and alice rather unwillingly took the place of the march hare. the dormouse went on, yawning and rubbing its eyes, for it was getting very sleepy; they drew all manner of things -- everything that begins with an m? 'i think i may as well go in at once. last in the beautiful garden, among the bright flower - beds and the cool fountains.\n",
            "Chapter 10\n",
            "Starting:\n",
            "CHAPTER VIII. The Queen's Croquet-Ground A large rose-tree stood near the entrance of the garden: the roses ...\n",
            "Characters: Alice, Queen, Gutenberg, the White Rabbit, the King.\n",
            "Key terms: 'Call the first witness,', RED, the Knave of Hearts, MINE, Panther.\n",
            "Quote: \"The chief difficulty Alice found at first was in managing her flamingo: she succeeded in getting its body tucked away, comfortably enough, under her arm, with its legs hanging down, but generally, just as she had got its neck nicely straightened out, and was going to give the hedgehog a blow with its head, it WOULD twist itself round and look up in her face, with such a puzzled expression that she could not help bursting out laughing: and when she had got its head down, and was going to begin again, it was very provoking to find that the hedgehog had unrolled itself, and was in the act of crawling away: besides all this, there was generally a ridge or furrow in the way wherever she wanted to send the hedgehog to, and, as the doubled-up soldiers were always getting up and walking off to other parts of the ground, Alice soon came to the conclusion that it was a very difficult game indeed.\"\n",
            "The queen's croquet - ground a large rose - ground a large rose - tree stood near the entrance of the garden: the roses growing on it were white. seven flung down his brush, and had just begun 'well, of all the unjust things--' of all the unjust things-- 'when his eye chanced to fall upon alice, as she stood watching them. the three gardeners were all shaped like the three gardeners, oblong and flat. alice was rather doubtful whether she ought not to lie down on her face like the three gardeners, but she could not remember ever having heard of such a rule at processions. two, in a very humble tone, going down on one knee as he spoke. 'i don't think it's at all a pity,' said alice. alice thought she had never seen such a curious croquet - ground in her life. the queen was going to give the hedgehog a blow with its head. the cat was looking about for some way of escape, and wondering whether she could get away without being seen. 'i don't think they play at all fairly,' alice began, in rather a complaining tone '' i should have croqueted the queen's hedgehog just now, only it ran away when it saw mine coming! 'i don't like the look of it at all,' said the king: 'i've read that in some book, but i don't remember where'  'i've read that in some book, but i don't look at me like that!'the queen had only one way of settling all difficulties, great or small. the hedgehog was engaged in a fight with another hedgehog. ) alice could think of nothing else to say but 'it belongs to the duchess: you'd better ask her about it.\n",
            "Chapter 11\n",
            "Starting:\n",
            "CHAPTER IX. The Mock Turtle's Story 'You can't think how glad I am to see you again, you dear old thing!' ...\n",
            "Characters: Alice, Gryphon, Queen, Soup, know--.\n",
            "Key terms: the Mock Turtle, Duchess, 'Call the first witness,', French, earth.\n",
            "Quote: \"'I quite agree with you,' said the Duchess; 'and the moral of that is--\"Be what you would seem to be\"--or if you'd like it put more simply--\"Never imagine yourself not to be otherwise than what it might appear to others that what you were or might have been was not otherwise than what you had been would have appeared to them to be otherwise.\"'\"\n",
            "The mock turtle's story 'you can't think how glad i am to see you again, you dear old thing!'the duchess was a little startled when she heard her voice close to her ear. the duchess: 'and the moral of that is--\"oh,' tis love, that makes the world go round! the duchess: 'what a clear way you have of putting things!'\"'' i think i should understand that better, 'alice said very politely. the duchess's voice died away, even in the middle of her favourite word 'moral. the queen merely remarking that a moment's delay would cost them their lives. alice heard the king say in a low voice, to the company generally, 'you are all pardoned'  'i must go back and see after some executions i have ordered,' she said. 'i never was so ordered about in all my life,' said the gryphon. alice thought to herself, 'i don't see how he can even finish, if he doesn't begin'  'i don't see how he can even finish, if he doesn't begin.'the mock turtle went on at last, more calmly, though still sobbing a little now and then, 'we called him tortoise because he taught us,' said the mock turtle angrily: 'really you are very dull!''i've been to a day - school, too,' said alice, 'we learned french and music'  'i've been to a day - school, too,' said alice, 'we learned french and music'  'i've been to a day - school, too,' said alice, 'we learned french and music'  'i couldn't have wanted it much,' said alice; 'living at the bottom of the sea.''i never heard of \"uglification,\"' alice ventured to say. the mock turtle: 'nine the next, and so on.'\n",
            "Chapter 12\n",
            "Starting:\n",
            "CHAPTER X. The Lobster Quadrille The Mock Turtle sighed deeply, and drew the back of one flapper across ...\n",
            "Characters: Alice, Gryphon, Soup, Beau, can--.\n",
            "Key terms: the Mock Turtle, 'Call the first witness,', the Lobster Quadrille, Seals, England.\n",
            "Quote: \"'You may not have lived much under the sea--' ('I haven't,' said Alice)--'and perhaps you were never even introduced to a lobster--' (Alice began to say 'I once tasted--' but checked herself hastily, and said 'No, never') '--so you can have no idea what a delightful thing a Lobster Quadrille is!'\"\n",
            "The lobster quadrille the mock turtle sighed deeply, and drew the back of one flapper across his eyes. seals, turtles, salmon, salmon, and so on; then, when you've cleared all the jelly - fish out of the way--. the mock turtle had been jumping about like mad things all this time. \"you can really have no notion how delightful it will be when they take us up and throw us, with the lobsters, out to sea!\"\"'' thank you, it's a very interesting dance to watch, 'said alice. the mock turtle: 'i never knew so much about a whiting before'  'i never knew so much about a whiting?'the gryphon went on in a deep voice, 'are done with a whiting. the mock turtle said: 'no wise fish would go anywhere without a porpoise'  'i mean what i say,' said the mock turtle: 'why, if a fish came to me, and told me he was going a journey, i should say \"with what porpoise?'the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide. \"'tis the voice of the lobster. ootiful soo, ootiful soo, oop soo, oop soo, oop soo and ootiful soo -- oop soo -- oop of the e -- e -- e -- evening, beautiful, beautiful soup! the melancholy words:-- 'soo was oop of the e -- e -- e -- evening, beautiful soup!\n",
            "Chapter 13\n",
            "Starting:\n",
            "CHAPTER XI. Who Stole the Tarts? The King and Queen of Hearts were seated on their throne when they ...\n",
            "Characters: Hatter, Alice, Dormouse, the King, the March Hare.\n",
            "Key terms: 'Call the first witness,', the Knave of Hearts, Fifteenth, 'The twinkling.\n",
            "Quote: \"The King and Queen of Hearts were seated on their throne when they arrived, with a great crowd assembled about them--all sorts of little birds and beasts, as well as the whole pack of cards: the Knave was standing before them, in chains, with a soldier on each side to guard him; and near the King was the White Rabbit, with a trumpet in one hand, and a scroll of parchment in the other.\"\n",
            "The king and queen of hearts were seated on their throne when they arrived. alice had never been in a court of justice before, but she had read about them in books, and she was quite pleased to find that she knew the name of nearly everything there. alice began in a loud, indignant voice, but she stopped hastily, for fear they should forget them before the end of the trial. the lizard) blew three blasts on the trumpet, and then unrolled the parchment scroll, and read as follows:-- 'the queen of hearts, she made some tarts, all on a summer day: the knave of hearts, all on a summer day. 'i beg pardon, your majesty,' said the king; and the white rabbit blew three blasts on the trumpet. the king exclaimed, turning to the jury, made a memorandum of the fact. 'i wish you wouldn't squeeze so,' said the dormouse, who was sitting next to her. the march hare interrupted in a great hurry. the miserable hatter dropped his teacup and bread - and - butter, and went down on one knee. 'i can't go no lower,' said the hatter: 'i'm on the floor, as it is' ' i'd rather finish my tea, 'said the king. the king looked anxiously at the white rabbit, who said in a low voice, 'your majesty must cross - examine this witness' ' i must, 'the king said in a low voice,' your majesty must cross - examine this witness'  the king, with an air of great relief. alice watched the white rabbit as he fumbled over the list.\n",
            "Chapter 14\n",
            "Starting:\n",
            "CHAPTER XII. Alice's Evidence 'Here!' cried Alice, quite forgetting in the flurry of the moment how ...\n",
            "Characters: Alice, the White Rabbit, the King, Gryphon, Wonderland.\n",
            "Key terms: 'Call the first witness,', the Mock Turtle, Adventures, Duchess.\n",
            "Quote: \"So she sat on, with closed eyes, and half believed herself in Wonderland, though she knew she had but to open them again, and all would change to dull reality--the grass would be only rustling in the wind, and the pool rippling to the waving of the reeds--the rattling teacups would change to tinkling sheep-bells, and the Queen's shrill cries to the voice of the shepherd boy--and the sneeze of the baby, the shriek of the Gryphon, and all the other queer noises, would change (she knew) to the confused clamour of the busy farm-yard--while the lowing of the cattle in the distance would take the place of the Mock Turtle's heavy sobs.\"\n",
            "Alice's evidence 'here!'the king said, turning to the jury, turning to the jury. 'i'm not a mile high,' said the king. the white rabbit was written by the prisoner to -- to somebody. the king, a general clapping of hands at the beginning, said the king. don't let him know she liked them best, for this must ever be a secret. the jury all wrote down on their slates, 'she doesn't believe there's an atom of meaning in it. the king triumphantly said the king triumphantly, pointing to the tarts on the table. the king said, for about the twentieth time that day. 'i've had such a curious dream!'the long grass rustled at her feet as the white rabbit hurried by. the grass would change to tinkling sheep, bells, and the pool rippling to the voice of the shepherd boy.\n",
            "Chapter 15\n",
            "Starting:\n",
            "End of Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll ...\n",
            "Characters: Gutenberg, Alice, Wonderland.\n",
            "Key terms: Adventures.\n",
            "Quote: \"End of Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\"\n",
            "End of project gutenberg's alice's adventures in wonderland, by lewis carroll\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdF8J1e7gPct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}